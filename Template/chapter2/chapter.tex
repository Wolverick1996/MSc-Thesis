% !TEX root = ../thesis.tex
%\chapter{State of the Art}
\chapter{Socio-Ethical Preliminaries}
\label{capitolo2}
\thispagestyle{empty}

\iffalse

This chapter discusses the state of the art that is relevant for your own work. What does that mean? It means that it provides the reader with all the relevant references he/she may need to know in order to understand better three things: (i) the context of your work, (ii) the problem and the need for a solution, and (iii) the value of your contribution. You achieve this by citing works or scientific papers that solved the same or similar problems in the past. Citing does not just mean adding a references to the bibliography and printing a number here; it means you tell the reader about the merits and possible demerits of each of the references you feel relevant. Of course, doing so requires you to first read each reference and, most importantly, to understand it. There should be lots of references in this chapter. 

It is advisable that you structure the chapter into sections in function of the topics you treat. If you do so, before starting with the first section of the chapter, explain the reader how you structure your discussion in one paragraph.

\begin{itemize}
\item[\Square] \emph{Read} relevant literature and or \emph{test} related software or tools.
\item[\Square] \emph{Summarize} your reading.
\item[\Square] Provide correct \emph{references} (the bibliography in the end of this document).
\end{itemize}

\fi


The aim of this chapter is to provide to the reader preliminary notions of ethical and sociological rather than technical nature.

Starting from the concept of \textit{bias}, passing through \textit{discrimination} and \textit{human rights}, we will discuss about \textit{equality}, \textit{equity} and finally \textit{fairness}, by providing definitions and relevant examples from the literature on these topics, with a focus on the data and computer systems perspective. It is important to emphasize that, despite the exhaustiveness of the definitions, these terms often have different meanings depending of the context of use, and there is a lot of debate on how to interpret them and eventually include all their dimensions in computer systems.

Because of the ``dual nature'' of this research, this chapter has to be seen as complementary to Chapter~\ref{capitolo3}, in which some technical bases will be provided, together with an overview on the tools adopted.


\section{Bias}
\label{section:bias}
Although the word ``\textbf{bias}'' does not have an intrinsically negative meaning (it is informally used to indicate a deviation from neutrality), it is mostly adopted in contexts where it entails a moral and social dimension. As reported in \cite{friedman2017bias}:
\begin{quote}\emph{We use the term bias to refer to computer systems that \emph{systematically} and \emph{unfairly discriminate} against certain individuals or groups of individuals in favor of others. A system discriminates unfairly if it denies an opportunity or a good or if it assigns an undesirable outcome to an individual or group of individuals on grounds that are unreasonable or inappropriate.} \cite[p.~332]{friedman2017bias}\end{quote}
Therefore, it is important to underline that unfair discrimination due to bias is strictly related to systematic and unfair outcome, where the word ``systematic'' is used with the meaning of ``regular, which occurs methodically when certain conditions arise''.

By following the classification provided in \cite{friedman2017bias}, we can distinguish three overarching categories of bias:
\begin{itemize}
\item \textbf{Preexisting bias}: it has its roots in social institutions, practices and attitudes. Preexisting bias may originate in the society at large or in subcultures and organizations (\textit{societal bias}), but it is also intrinsic in the nature of every human being (\textit{individual bias}), and can enter a computer system either voluntarily or implicitly and unconsciously, even in spite of the best intentions of the system designer. Furthermore, since preexisting bias is often related to historical discrimination of disadvantaged groups, it could lead to the introduction or the exacerbation of representation issues in the data.
An example of preexisting (gender) bias is the one present in the society that leads to the development of educational software that overall appeals more to boys than girls \cite{friedman2017bias}.
\item \textbf{Technical bias}: it arises from the resolution of issues in the technical design. Technical bias may originate from design choices, constraints and technological tools, or exacerbate preexisting bias.
An example of technical bias, due to technical constraints, is the one of a monitor screen displaying the flight options most relevant to an airline customer: the screen dimension forces a piecemeal representation of the flights and therefore if the ranking algorithm systematically places certain flights on initial screens and other flights on later screens, it exhibits technical bias \cite{friedman2017bias}.
\item \textbf{Emerging bias}: it emerges some time after a design is completed, as a result of changing societal knowledge, population, or cultural values. Emerging bias is strictly related to the specific context of use, and it is the most difficult to detect.
An example of emerging bias, caused by a \textit{mismatch between users and system design} due to \textit{different values} (that is, originated when a computer system is used by a population with different values than those assumed in the design), is the one of an educational software embedded in a game situation that rewards individualistic and competitive strategies used by students with a cultural background that eschews competition and promotes collaboration \cite{friedman2017bias}.
\end{itemize}

For the purpose of this research, we will focus on preexisting bias (in particular, societal bias) and technical bias, but it is important to point out that emerging bias should not be underestimated in the long run, especially when it arises from a mismatch between users and system design due to different values, because society is in constant change and systems should be readjusted or reinvented in order to keep up with the present.

A significant example of preexisting (racial) bias, exacerbated by technical bias, is provided in \cite{angwin2016machine}: a commercial tool called COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) was used in courts in the U.S. to automatically predict some categories of future crime to assist in bail and sentencing decisions. On average, the tool correctly predicted recidivism 61\% of the time, but blacks were almost twice as likely as whites to be labeled a higher risk but not actually re-offend. The tool made the opposite mistake among whites: they were much more likely than blacks to be labeled lower risk but go on to commit other crimes.

Other examples, related to gender bias and technology, are given by \cite{gibbs2015women} and \cite{dastin2018amazon}. The former is about a research conducted in 2015, in which a tool was used to simulate job seekers that did not differ in browsing behavior, preferences or demographic characteristics, except in gender. One experiment showed that Google displayed adverts for a career coaching service for ``\$200K+'' executive jobs 1,852 times to the male group and only 318 times to the female group. The latter concerns another Big Tech company, Amazon, whose machine learning specialists, back in 2015, discovered that their new recruiting engine was not rating candidates in a gender-neutral way, because the system taught itself that male candidates were preferable by penalizing resumes that included the word ``women's''.


\section{Discrimination}
Bias can lead to \textbf{discrimination}, but what discrimination is and how it occurs is a controversial issue. As reported in \cite{scantamburlo2018machine}, often the law, rather than providing a definition of discrimination, defines a list of attributes, called \textbf{protected attributes}, that cannot be used to take decisions in various settings. The list is non-exhaustive and includes characteristics such as race, gender, religion, or sexual orientation. Groups of people that are more likely to be discriminated against because of these attributes are therefore classified as \textit{protected groups}.

Trying to elaborate a bit more, we can define discrimination as the result of either one or both the following:
\begin{itemize}
\item \textbf{Disparate treatment}, or \textit{intentional discrimination}: the illegal practice of treating an entity, such as a job applicant, differently based on a protected attribute such as race, gender, age, religion, sexual orientation or national origin because of a discriminatory motive.
\item \textbf{Disparate impact}, or \textit{unintentional discrimination}: the result of structural disparate treatment, in which policies, practices, rules or other systems that appear to be neutral result in a disproportionate adverse impact on a protected group. Disparate impact is not based on a discriminatory motive and the discriminating agent is usually unaware of the discrimination.
\end{itemize}

Protected attributes are mentioned in the article 2 of the Universal Declaration of Human Rights (UDHR), which states:
\begin{quote}\emph{Everyone is entitled to all the rights and freedoms set forth in this Declaration, without distinction of any kind, such as race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth or other status.} \cite{assembly1948universal}\end{quote}
Discrimination is therefore strictly related to \textit{human rights}, together with the concept of \textit{equality}, since ``equal'', ``equally'' and ``equality'' itself are recurring words in several articles of the Declaration.


\section{Human Rights}
For what concerns \textbf{human rights}, there is a lot of debate on how to incorporate them in computer systems by following a ``human-rights-by-design'' approach \cite{penney2018advancing}, in order to contrast the negative effects of the so called ``dual-use technologies'': products which may serve legitimate societal objectives but are also used to undermine human rights like freedom of expression or privacy. Of course, reaching this goal would require a culture shift and huge efforts from both national governments and businesses, which should design tools, technologies, and services to respect human rights by default, rather than permit abuse or exploitation of them.
A similar concept is proposed in \cite{yeung2020ai}, where the authors sketch the contours of a comprehensive governance framework for ensuring AI systems to be ethical in their design, development and deployment, and not violate human rights. This framework should be effective in contrasting \textit{ethics washing}: the practice of fabricating or exaggerating a company's interest in equitable AI systems that work for everyone, a sort of side door that companies use to substitute regulation with ethics.

For the purpose of this research, we can define human rights as ``inalienable fundamental rights to which a person is inherently entitled simply because she or he is a human being'' \cite[p.~3]{sepuldeva2010human}. A few examples are the rights to life and liberty, freedom from slavery and torture, freedom of opinion and expression, the rights to work and education, and the right to the pursuit of happiness. These norms are concerning every human being, regardless of sex, age, language, religion, ethnicity, or any other status.


\section{Equality \& Equity}
\label{section:equality_equity}
Equality is generally intended as ``an ideal of uniformity in treatment or status by those in a position to affect either'' \cite{britannica2009equality}. The concept of equality is often associated with discrimination mostly because of the article 7 of the UDHR, which states:
\begin{quote}\emph{All are equal before the law and are entitled without any discrimination to equal protection of the law.} \cite{assembly1948universal}\end{quote}
This principle is known as ``equality before the law'', and establishes that everyone must be treated equally under the law regardless of race, gender, color, ethnicity, religion, disability, or other characteristics, without privilege, discrimination or bias.

However, it is important to distinguish between two different political and social theories:
\begin{itemize}
\item \textbf{Equality of opportunity}:
\begin{quote}\emph{The idea that people ought to be able to compete on equal terms, or on a ``level playing field'', for advantaged offices and positions.} \cite{britannica2019equal}\end{quote}
This principle is based on the notion of \textit{sameness}, where fairness is achieved through equal treatment regardless of people's needs. Equality of opportunity is usually simply referred as \textbf{equality}, and from now on we will adopt the same terminology for this research.
\item \textbf{Equality of outcome}: the idea that people should have access to resources (possibly of a different nature and to a different extent) in order to be able to reach the same condition. This principle is based on the notion of \textit{need}, where fairness is achieved by treating people differently depending on their endowments and necessities. Equality of outcome is also known as \textbf{equity}, and from now on we will adopt the same terminology for this research.
\end{itemize}

\begin{figure}[h!]
\includegraphics[scale=.5]{figures/equality_vs_equity.jpg}
\centering
\caption{Visual example of the difference between equality and equity.\newline
\copyright 2014, Saskatoon Health Region. Source: \upshape\protect\url{https://www.nwhu.on.ca/ourservices/Pages/Equity-vs-Equality.aspx}.}
\label{fig:equality_vs_equity}
\end{figure}

Figure~\ref{fig:equality_vs_equity} provides a simple example of the difference between equality and equity. Treating people equally, in this scenario, means to give everyone the same one box to reach the fruit, while treating people equitably means to give them as many boxes as they need to achieve the goal. It is important to notice that equity could require (and often requires) unequal treatment.

Moving on to the data perspective, we can now extend (or restrict, depending on the point of view) the equality and equity concepts to data equality and data equity. Data equality usually refers to transparency of institutions and companies towards customers regarding the information collected on their account, whereas data equity is used in a different context. The authors of \cite{jagadish2021facets} distinguish between four different facets of data equity:
\begin{itemize}
\item \textbf{Representation equity}: bias may arise because of material deviations between the data and the world represented by the data, often with respect to historically disadvantaged and underrepresented groups. Even when dealing with contemporary data, disparities rooted in historical discrimination can lead to representation inequities and therefore to the introduction or the exacerbation of problems. For example, in the U.S. there has been a lot of discussion about racial disparities concerning COVID-19, regarding both availability of testing (fewer test sites in minorities neighborhoods, historically poorer) and desire of individuals to be tested (black people more suspicious about the medical system, because of their history of unfair treatments) \cite{jagadish2021facets}. Another example, already mentioned in Section~\ref{section:bias}, is related to Amazon and a software developed by the company for screening candidates for employment: the software was trained on the already hired employees and since they were mostly males, females became underrepresented in the data and the software was much more likely to mark women as unsuitable for hiring.
\item \textbf{Feature equity}: bias may arise because not all the features needed to represent a marginalized group of people and required for a particular analysis are available in the data, or because some of these features are voluntarily removed in the decision-making process. As an example, for a specific study involving transgender people, it may be important to distinguish between their birth name and their self-assigned name.
\item \textbf{Access equity}: bias may arise because of a non-equitable and participatory access to data and data products across domains and levels of expertise due for instance to the opacity of data systems or the need to respect the privacy of data subjects. A classical example is the one of medical records: making them public could lead to the development of new techniques to eradicate diseases, but on the other side most people are very sensitive about sharing medical information because of the simplicity of re-identify anonymized data, and there are a lot of regulatory constraints on such sharing.
\item \textbf{Outcome equity}: bias may arise because of a lack of monitoring and mitigation of unintended consequences for any group affected by the system after deployment, directly or indirectly (for example, contact tracing apps may facilitate stigma or harassment).
\end{itemize}


\section{Fairness}
As discussed in Section~\ref{section:equality_equity}, both equality and equity aim to achieve \textbf{fairness}, despite the different approaches of the two theories, but what fairness really is is a widely debated topic. A very generic definition, taken from \cite{cambridge2013fairness}, depicts it as ``the quality of treating people equally or in a way that is right or reasonable''.

In the sociological context, fairness is often seen as a synonym of \textit{justice}, and consequently \textbf{social justice} is fairness as it manifests in the society, described by \cite[p.~405]{barker2003social} as ``an ideal condition in which all members of a society have the same rights, protections, opportunities, obligations, and social benefits''. Although the literature on this subject does not always agree on their number, we can delineate five interrelated principles of social justice, by following the classification provided in \cite{corporate2020social}:
\begin{itemize}
\item \textbf{Access to resources}: a just society should provide services and resources that are available to each different socioeconomic group, in order to give everyone an equal start in life.
\item \textbf{Equity}: in unjust societies, there are always disenfranchised groups. These groups need to receive more support from the society than privileged ones, in order to move towards the same outcome.
\item \textbf{Participation}: everyone in a just society, and not just small groups of individuals, should be able to participate in the decisional processes that affect their lives.
\item \textbf{Diversity}: a just society should recognize the value of diversity and cultural differences, and develop ad-hoc policies with the aim of breaking down societal barriers.
\item \textbf{Human rights}: a just society should ensure the protection of everyone's civil, political, economic, cultural, and social rights.
\end{itemize}

Moving back to the data and computer systems perspective, and recalling the aforementioned concepts of equality and equity, we can distinguish between two different concepts of fairness \cite{dwork2012fairness}:
\begin{itemize}
\item \textbf{Individual fairness}: any two individuals who are similar \textit{with respect to a task} should receive similar outcomes. The similarity between individuals should be captured by an appropriate metric function, usually difficult to determine. For example, deciding whether or not to display a specific advertisement is a classification task, and the definition of individual fairness assumes the existance of a task-specific metric (e.g. the number of clicks made by the users) capable of determining, for any two individuals, how (dis)similar they are for the specific task. Individual fairness is strictly related to the idea of equality.
\item \textbf{Group fairness} (also known as \textit{statistical parity}): demographics of the individuals receiving any outcome - positive or negative - should be the same as demographics of the underlying population. For example, in the problem of predicting if hiring applicants, assuming to divide them into groups according to their gender, this means the acceptance rates of the applicants from the groups must be equal regardless of the protected attribute. Group fairness equalizes outcomes across protected and non-protected groups, and is therefore strictly related to the idea of equity.
\end{itemize}

Although individual and group fairness are not mutually exclusive in theory, in real life it is often hard to conciliate the two approaches. Furthermore, this categorization is not the only possible one: the authors of \cite{verma2018fairness} collected and provided about twenty among the most prominent definitions of fairness, and applied each of them to a case study based on gender-related discrimination, in which the aim was to assign a credit score to people requesting a loan by using ``Personal status and gender'' as a protected attribute for the decision-making process, operated by a classifier (an algorithm that automatically orders or categorizes data into one or more of a set of ``classes'', in this case only ``good credit score'' and ``bad credit score'').

Among the others, a couple of peculiar definitions, often listed together with individual and group fairness, are the following:
\begin{itemize}
\item \textbf{Fairness through unawareness}: protected attributes are not used in the decision-making process, and therefore the subsequent decisions cannot rely on them. This ``blind'' approach relies on \textit{impartiality} and is consistent with the disparate treatment principle, but removing features means losing information, and furthermore there could be features correlated to protected attributes that would not be removed, potentially introducing bias.
\item \textbf{Counterfactual fairness}: a precise and non-technical definition is provided in \cite{wu2019counterfactual}:
\begin{quote}\emph{A model is fair if for a particular individual or group its prediction in the real world is the same as that in the counterfactual world where the individual(s) had belonged to a different demographic group. However, an inherent limitation of counterfactual fairness is that it cannot be uniquely quantified from the observational data in certain situations, due to the unidentifiability of the counterfactual quantity.} \cite[p.~1]{wu2019counterfactual}\end{quote}
To better clarify the concept, we could imagine a situation in which a software has the task of deciding whether or not to assign a promotion to the employees of a company by looking at their profile that includes, among the other attributes, sex and race. The software is counterfactually fair if, for each individual, the outcome of the analysis is the same both in the case in which the real values of these attributes are used and in the case in which these values are replaced with others (counterfactuals).
\end{itemize}

The classifier resulted to be fair depending on the notion of fairness adopted, showing the impossibility of addressing fairness as a unique, broad and inseparable concept. This result is coherent with \textit{Chouldechova's impossibility theorem} \cite{chouldechova2017fair}, which demonstrates, taking three definitions of fairness, the impossibility of satisfying all of them.


\iffalse

\section{Summary}
Close the state of the art chapter with some words that connect the discussion of the references to your thesis. Pay attention that the reader understands why you discussed the works/topics you discussed and how they are related to what you do.

\begin{itemize}
\item[\Square] Show that in the state of the art the \emph{problem} you want to solve has not yet been solved or not been solved in an as efficient / effective / easy to use / cost-saving fashion as you target with your work.
\item[\Square] If your work has similarities with some \emph{specific references}, point them out here and explain why these are particularly important to you. Perhaps you started your investigation from the outputs of a specific paper or you want to improve the performance of an algorithm studied earlier; it's good to mention this here.
\item[\Square] Attention: this is not yet the place where to anticipate \emph{your solution}. You may give hints, but it's too early to make a comparison between your work and the state of the art, as the reader does not yet know anything about your work. This discussion can go into the final chapter.
\end{itemize}

\fi
