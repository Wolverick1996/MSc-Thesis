% !TEX root = ../thesis.tex
\chapter{Conclusions \& Future Work}
\label{chapter:conclusions_future_work}
\thispagestyle{empty}

The aim of this chapter is to draw the conclusions of our research, mixing the results obtained in our experiments described in Chapter~\ref{chapter:experiments} with the sociological background depicted in Chapter~\ref{chapter:sociological_research}, and recalling some preliminaries exposed in Chapter~\ref{chapter:socio-ethical_preliminaries} and Chapter~\ref{chapter:technical_preliminaries} when needed.

We will start by recapitulating our work in a \textit{conclusive summary}, and then we will discuss about the \textit{outcomes and contributions} of our research, emphasizing also the \textit{limitations} to which we were subjected. Finally, we will suggest \textit{future work} to be potentially done starting from this research, and some possible paths that might be worth to follow.


\section{Conclusive Summary}
Starting from the general notions of fairness and data management, we reviewed both socio-ethical and technical literature in order to dive deep into the topics and capture the several facets of the problem of discovering bias in the data.

For what concerns the socio-ethical side, we classified three categories of bias, different in origin and in nature; we clarified what discrimination is, and how it is connected to human rights; and we distinguished between equality and equity, extending the concepts to the data perspective, and finally pointing at what both approaches aim to achieve: fairness.

On the technical side, we first introduced the basics of the discipline by providing explanations on relational databases, data science pipeline, and data mining techniques; we then explored some more specific concepts such as linear regression and functional dependencies, also providing detailed information on some evaluation metrics for them, and on some other useful statistical concepts; and we finally described, referring to the documentation at our disposal, the tools we decided to adopt for our analysis: the ``Glassdoor Method'', FAIR-DB, and Ranking Facts.

After this first literature review phase, we conducted parallel research in sociology and information technology.

Once chosen the datasets on which to work and the specific focus of our analysis -- gender gap -- we retrieved information on the problem in the society, overall at first, introducing The Global Gender Gap Index, and then focusing on the U.S., providing different point of views and reasons for the presence of gender discrimination in the workplace, such as statistical discrimination, impact of the institutional environment, and inequalities related to unequal bargaining power; and finally we showed some data and statistics from the U.S. Department of Labor, in order to make more concrete the reflections previously reported.

Meanwhile, we did some experiments on our datasets, in order to verify the possible presence of bias in the data, potentially leading to gender discrimination and unfair outcomes. For both our case studies, after having provided a more detailed description of the dataset, we performed some data preprocessing operations; and we used the tools previously described to analyze them. We tried to identify -- both during the preprocessing and the actual analysis -- the most critical choices we had to deal with, pointing them as potential sources of bias; and we conducted other experiments on the same data but taking different paths, ultimately evaluating the impact of different design decisions on our outcomes.


\section{Outcomes \& Contributions}
Combining the sociological research with the technological results of our analysis, we can highlight two main issues: a \textit{representation problem}, related to the disproportion in the percentage of women employed in different sectors, and a \textit{part-time problem}, due to the higher number of women employed in part-time jobs, typically less paid than full-time ones.

For what concerns representation, we want to recall the data and statistics provided in Section~\ref{section:data_statistics_dol} related to the most common occupations for women and the percentage of women employed in STEM disciplines, and point out that, even though there is clear evidence of the presence of this problem in our datasets, as we have mentioned throughout Chapter~\ref{chapter:experiments}, none of the adopted tools highlights this particular aspect (Ranking Facts tries to encompass it through its diversity widget, with the result of providing just a high-level and absolutely non-exhaustive visual representation that cannot capture the underlying complexity and the different facets of the issue).

The part-time condition, for which we recall Figure~\ref{fig:dol_full-_and_part-time_workers_by_sex} and, in general, the reflections made in Chapter~\ref{chapter:sociological_research}, is instead more perceptible from the tools, since the information is encoded in our datasets under the \textit{Status} attribute, and therefore could be used for estimating the ``adjusted'' pay gap in the ``Glassdoor Method'', as constituent of functional dependencies in FAIR-DB, and as a ranking parameter in Ranking Facts. Removing the part-time employees instead resulted to be penalizing both from the technical point of view, as underlined in Section~\ref{section:part-time_employees_removal}, and from the sociological one, given the background provided in Chapter~\ref{chapter:sociological_research}.

The most impactful limitation of the tools, in any case, is the fact that even though they all aim to achieve fairness not only through equality but also by taking equity into account (by privileging the group fairness criteria over the individual fairness one), they practically fail in capturing the several facets of equity. Recalling the classification made in Section~\ref{section:equality_equity}, we proved how inefficient these instruments are in capturing \textit{representation equity} (disparities rooted in historical discrimination can lead to representation inequities); \textit{feature equity} (not all the features needed to represent a marginalized group and required for a particular analysis are available in the data) is not addressed at all, since the tools do not have knowledge on the context, and therefore they cannot, for example, infer what the relevant attributes for the specific analysis may be and inform the user on what is missing; and neither are addressed \textit{access equity} (bias may arise because of a non-equitable and participatory access to data and data products across domains and levels of expertise) and \textit{outcome equity} (bias may arise because of a lack of monitoring and mitigation of unintended consequences for any group affected by the system after deployment).

Moving from outcomes to contributions: first of all we confirmed, in accordance with Chouldechova's impossibility theorem \cite{chouldechova2017fair} and however trivial it may be, that fairness is a multifaceted concept which cannot be exhausted by providing a single definition and pursuing that specific definition experimentally. In this context, we believe that the simultaneous use of several tools for the same analysis represents an undoubted advantage, since each of them provides a different approach and hence a different perspective on the same problem.

We also demonstrated how these ``fairness measurement tools'' are susceptible to decisional choices, and therefore how important it is to properly train users on the specific area of analysis. A question that might arise is: ``since human intervention (which takes the form of choices in the use of these tools) necessarily introduces, being human, bias, does it make sense to keep humans in the loop?''. On the other hand, we could ask ourselves: ``given the problems involved in developing a universal instrument that can address the issue in all its facets, does it make sense to manage decision-making processes with these tools?''. We claim the answer to be yes in both cases: fairness indeed is a complex human concept of abstract nature, with impactful concrete social implications, and as such it requires a non-automatable human intervention; on the other hand it is not possible to do without the tools -- that it is necessary to continue to build and improve -- since the amount of data to handle is absolutely excessive and not manageable otherwise.

Lastly, we believe that the main contribution of this research is given by the double perspective on the gender pay gap issue, which we hope will set a precedent for data scientists and the other professionals in the IT sector for approaching other social problems. We strongly believe in multidisciplinarity and in the potentials of facing challenges not only by looking at the knowledge available in one's field of study but also in other, possibly interconnected, disciplines.


\section{Limitations}
\label{section:limitations}
Although our research provided us with some significant results, there are of course aspects of our work which limit the impact or generalizability of our contributions.

From the sociological point of view, the most important limitation is given by the fact that the literature of reference is usually non-specific, and although the majority of our sources is related to the U.S. society, the United States is a very wide country which also presents significant differences between the various states that make it, and none of the papers specifically refers to Illinois or California, and neither goes into the detail of the cities of Chicago or San Francisco. Furthermore, we had to rely on publicly available data and resources, and we did not get any insight from employees or employers working in the U.S., so our view on the overall situation may be partial.

From the technical perspective, we already mentioned throughout Chapter~\ref{chapter:experiments} some of the most impactful choices we had to deal with, which potentially lead to the introduction of bias, but we will now recapitulate providing a more exhaustive summary.
\begin{itemize}
\item The original datasets are already partial, because they contain a limited number of job titles, and we are not aware of possible preexisting groupings of employees who may work on some specific tasks but be grouped in the same category, or of external employees who just temporarily work for the federal government.
\item The \texttt{gender-guesser} package we used to infer employees' gender is obviously not 100\% accurate, and even if we assumed mostly male names to be effectively related to males and mostly female names to be effectively related to females, we may have been mistaken in some cases. Furthermore, the package produced a non-negligible number of unknown and androgynous occurrences, possibly related to employees of various ethnicities with not typically Western names, and even if we decided to remove them in order to work on more reliable data, they may have had an impact on the results.
\item The data cleaning processes we performed by removing job titles with less than 100 occurrences significantly reduced the number of tuples of our datasets, and even if this choice led to a reduction of complexity, the downside is that a lot of categories of employees were excluded from the analysis.
\item As previously pointed out, none of the adopted tools takes into account overrepresentation of men (or underrepresentation of women) in the specific job title in which they are employed, and although we can see it is as an outcome for the purpose of our research, it is certainly also a limitation, being a hidden bias source.
\item The parameter values required for the FAIR-DB analysis have an impact on the number of dependencies detected and selected, and even if we used what we believe to be the best values, a deeper knowledge of the underlying concepts may have led to a refinement of the results. The same holds for the manual selection of the rules, that is a crucial phase of the framework, with the power to overturn the final results and for which, given the lack of documentation, we adopted what we think is the most sensible and suitable criterion. Choosing instead all the detected rules, as shown in Section~\ref{section:fair-db_choice_different_dependencies}, strongly impact the final outcomes, reversing the perspective on the fairness of data.
\item Given the lack of documentation for Ranking Facts, we followed the approach of the authors of the tool by trying to laboriously follow the examples they provided in setting up the ranking parameters and their weights, but again a deeper knowledge of the underlying concepts may have led to a refinement of the results. Furthermore, given the size of our datasets, we were forced to use the notebook version of the tool, which is undoubtedly less user-friendly than the Web-based application.
\item The number of bins used for the FAIR-DB analysis and the values specified for the thresholds have an impact on the results. We tried to overcome this limit by conducting two different analyses, with respectively 2 and 8 bins (Section~\ref{section:fair-db_discretization_8_bins}), but other choices may have led to different outcomes.
\item As already mentioned in Section~\ref{section:grouping_job_titles}, for grouping job titles we relied on a document published by the Equality Commission for Northern Ireland in 2013 \cite{equality2013index}. Despite the exhaustiveness of the index, it is important to underline that the document was not made with the purpose of being used for grouping job titles outside Northern Ireland.
\item As already mentioned in Section~\ref{section:voluntary_introduction_of_bias}, the voluntary introduction of bias in our data produced a synthetic dataset, that is, a dataset containing fake data, not reflecting the real world in which we live.
\end{itemize}


\section{Future Work}
Assuming we have more time (and maybe more resources) at our disposal, here we want to highlight some aspects which we think would deserve further study or development in future research.

First of all, having seen the different approaches of the adopted tools and the different perspectives they provide, it would be interesting to combine them all in a unique, more complete instrument, in order to give the user a single tool that provides multiple points of view, rather than several partial ones which the user themselves may not combine, maybe because not aware of the existence of each. Further efforts may be invested in trying to encompass even more facets of equity, or more definitions of fairness.

Even assuming such an instrument is developed, however, analyses of these kind should always be supported by sociological research, in order to get a broader perspective on the problem and capture facets which would not be captured otherwise (in our case, the representation issue).

The sociological research could also be further enriched by conducting an interview with workers and HR practitioners of the cities under study, in order to get more specific, recent, and precise information useful for interpreting the results.

For what concerns datasets, it would be appropriate to retrieve further information in support of the mere data, in order to get a more exhaustive overview, and since since having more information usually leads to more accurate results. This also could be an incentive for developers, database administrators, and other professionals in the IT sector, to create effective documentation in support of data and technological tools. This documentation should, as far as possible, be detailed and at the same time easy to experience, so as to potentially enable professionals from other sectors (for example, sociologists) to get an idea of what is included in the data or how to use a tool in a conscious way. In this regard, we think that context-awareness would be an interesting path to follow, and some techniques may be used to provide the tool(s) with knowledge on the context of use. We believe that such an improvement would mitigate (even if not extinguish) problems, like the representation one, encapsulated in the data but not currently detectable. It is worth reporting here a couple of contributions that we think may be good starting points for addressing data quality issues. In particular, in \cite{canali2020towards}, Canali argues that:
\begin{quote}\emph{Quality is a contextual feature of data: it is a result of the relations established between a dataset and the questions, aims and tools employed in the context of the use of data; the assessment of the quality of a dataset needs to focus on the features of this context as much as the dataset itself.} \cite[p.~4]{canali2020towards}\end{quote}
He then delineates some guidelines according to which the contextual approach indicates quality criteria and assessment methods, and provides three practical examples in support of it. In \cite{leonelli2017global} instead, Leonelli examines some models of data quality evaluation that have been employed within the sciences, highlighting strengths and weaknesses of each and ultimately emphasizing again the importance of the context and of having exhaustive metadata in support of the mere data, in order also to facilitate international cooperation and the development of different projects on the same, accessible, data.
