% !TEX root = ../thesis.tex
\chapter{Introduction}
\label{chapter:introduction}
\thispagestyle{empty}

Before going into the contents of the research, we would like to specify that this thesis work was carried out in France at the \'Ecole normale sup\'erieure in Paris, under the supervision of Pierre Senellart, Professor of Computer Science at the aforementioned university, and Karine Gentelet, Associate Professor in the Department of Social Sciences at the Universit\'e du Qu\'ebec en Outaouais (Canada) and holder of the 2020-2021 Research Chair in AI and Social Justice at the \'Ecole normale sup\'erieure. Furthermore, this work has been funded by the French government under management of Agence Nationale de la Recherche as part of the `Investissements d'avenir' program, reference ANR-19-P3IA-0001
(PRAIRIE 3IA Institute).

This thesis work falls somewhere between the realm of data analysis -- a branch of data science and, more generally, of computer science and engineering -- and the realm of gender discrimination -- a multifaceted and debated field in ethics and especially in sociology.

The main contributions of this research derive from the application of preprocessing techniques and from the use of three tools created with the aim of detecting bias in the data, with which we try to understand, by carrying out some experiments, which design choices have the greatest impact on the so-called `fairness' of the results, and of which we highlight strengths and weaknesses, showing that a multidisciplinary approach to a given type of problem is of fundamental importance for a broader understanding of the problems themselves, and how important is, therefore, the collaboration of professionals in different fields and the combination of different working methodologies.

To pursue this goal, we focus on defining concepts that are fundamental to the understanding of the context of our work from both the socio-ethical and technological sides, and then we combine the more qualitative approach of research in sociology with the more technical approach of experiments in computer science.


\section{Research Context: Gender Discrimination in Data Analysis}
This research would like to be a bridge between two disciplines of very different nature, and which at first sight would seem to have little to do with each other: \textit{data science} and \textit{sociology}. As for the former, our focus is on \textit{data analysis}, that is, the set of processes for inspecting, cleaning, transforming, and modeling data with the aim of discovering useful information, informing conclusions, and supporting decision making. For what concerns the latter, we focus on \textit{social justice}, and even though most of the definitions are provided in further chapters, it is appropriate to clarify the very fundamental concepts behind our research, starting from the notion of `social problem'.
\begin{quote}\emph{Social problem is a generic term applied to a range of conditions and aberrant behaviors which are manifestations of social disorganization. It is a condition which most people in a society consider undesirable and want to correct by changing through some means of social engineering or social planning.} \cite{marschall1998oxford}\end{quote}

In our research, we focus on a specific category of social problems, namely those related to discrimination, and in particular on \textit{gender discrimination}, expressed in the form of the so-called `\textbf{gender gap}'. According to \cite{cambridge2013gender}, gender gap is definable as:
\begin{quote}\emph{A difference between the way men and women are treated in society, or between what men and women do and achieve.} \cite{cambridge2013gender}\end{quote}

Specifically, the focus of our experiments is \textit{gender pay gap}, that is, the average difference between the remuneration for men and women in the workforce, or, in other words, a measure of what women are paid relative to men. In our experiments we test some tools to verify not only whether or not men and women are paid fairly, but also if there are other collateral issues related to gender discrimination in the data and what tools can possibly catch them.
Our experiments are centered on the economical perspective because it is the easiest to be measured in the data, being quantifiable for example as a number representative of a person's average monthly salary, but it is important to keep in mind that other facets of the gender gap problem come into play when dealing with sociological studies.

For the aforementioned reasons, this research has an ethical and social impact, since it concerns processes that affect people's choices and lives, both individually and collectively. If ethical reflection (which concerns the moral dimension of people) is often criticized for being very abstract and for giving normative indications (that is, indications on how people should behave) far from practice, in this thesis work this dimension -- discussed in particular with respect to the concept of fairness -- is mitigated by the presence of sociological analysis, less normative and more practical.


\section{Scenarios \& Problem Statement}
Nowadays, digital devices have become pervasive in every aspect of our daily lives and almost every action we perform leaves a digital trail. We generate data whenever we go online, when we communicate with people through any kind of application, when we shop, or even just when we carry our smartphones. It is therefore of societal and ethical importance to ask whether data and datasets, on which so many actions of our daily routine are based, are \textit{fair} or not. Unfair, or better to say, \textit{biased} data (that is, data not representative of the entire population and that could potentially generate discriminatory outcomes), may in fact influence, directly or indirectly, our perception of reality, and lead us to make decisions that, although seemingly fair and just, contain in turn bias, and therefore discriminate against individuals or groups of individuals. Without going into too much detail, which are deepened in the following chapters, we now provide the reader with a couple of scenarios that can intuitively give the idea of the problem.

A first example, related to racial discrimination, is given by \cite{ledford2019millions}. A study conducted by the University of California in 2019 concluded that an algorithm used to allocate health care to patients in U.S. hospitals was less likely to refer Black\footnote{In this work, we chose to capitalize `Black', `White', and other analogous terms when referring to race and ethnicity simply because our data and sociological insights refer to the U.S. society, and hence we decided to adopt the current U.S. language convention.} people than White people who were equally sick to programs that aim to improve care for patients with complex medical needs. Although there was no discriminatory intent, misconceptions in the design phase led to the introduction of bias, and consequently to the involuntary discrimination of millions of Black citizens.

Another example, more closely linked to the focus of our research, namely gender discrimination, is provided in \cite{horwitz2021facebook}. The article reports the outcomes of a study led by the University of Southern California researchers in 2021, who found that Facebook systems were more likely to present job ads to users if their gender identity mirrored the concentration of that gender in a particular position or industry. Specifically, the team of researchers bought ads on Facebook for delivery driver job listings that had similar qualification requirements but for different companies: Domino's, who has more male drivers, and Instacart, who has more female ones. The study found that Facebook targeted the Instacart delivery job to more women and the Domino's delivery job to more men. Similar findings were obtained by testing software engineer job listings for Nvidia and Netflix, and therefore the researchers, in their conclusions, spoke of ``a platform whose algorithm learns and perpetuates the existing difference in employee demographics''.

Further examples and explanations are provided in the following chapters, but what we want to clarify is that our goal is not to solve the problem of discrimination in data, which is a huge and multifaceted issue which encompasses a human dimension very difficult to address, but rather to take a look at the current state of the art, observing some tools in action and trying to highlight their strengths and weaknesses, and also providing a non-technical perspective to give a broader picture of the situation by investigating the problem in the sociological field to obtain additional information about the context in which data are embedded and to understand to what extent the problem is to be found in the instrumentation and technical choices and to what extent it is instead rooted in society.
We believe that this research may represent a good starting point for those who work in the sector to have an overview of the problem and to better understand what to focus on, and we want to emphasize the importance of a multidisciplinary approach to deal with these complex and multifaceted issues, since the technical perspective alone is partial and not sufficient to understand phenomena of a social nature, which are reflected and may be exacerbated by technology but which in fact originate in society.


\section{Methodology}
This research, from the methodological point of view, combines the more qualitative and conceptual approach typical of sociology with the more technical and pragmatic approach of engineering and computer science.

We address the problem by first providing the reader with some preliminary socio-ethical and technical concepts taken from a \textit{systematic literature review}. Getting a little more specific, although precise definitions and further details are given in subsequent chapters, on the socio-ethical side we start by discussing the different facets of bias and clarifying what is meant by discrimination and how it relates to human rights; we then distinguish the often misused terms of equality and equity; and we finally point at what both equality and equity aim to achieve: fairness. Instead, on the technical side we first introduce the basics of the data analysis discipline by providing explanations on relational databases, data science pipeline, and data mining techniques; we then explore more specific concepts such as linear regression and functional dependencies, also providing collateral information on some evaluation metrics and useful statistical concepts; and we finally describe, referring to the documentation at our disposal, the tools we decided to adopt for our analysis.

After that, we provide a dual perspective in sociology and information technology, distinguishing some \textit{case studies} for our experiments and trying to interpret our results also looking at the sociological background introduced beforehand.
The experiments themselves are basically \textit{comparative studies} of the algorithms, and, in terms of \textit{software instruments}, we rely mainly on Python and Jupyter Notebook, with the various packages and libraries made available by developers.


\section{Thesis Structure}
We provide here a brief overview of the contents of each chapter, in order to help the reader move through the thesis and find specific information. The beginning of each chapter provides more detailed descriptions about the contents of the chapter itself.

\begin{itemize}
\item Chapter 2: \textbf{Socio-Ethical Preliminaries}. The aim of this chapter is to provide the reader with preliminary notions of ethical and sociological (rather than technical) nature, in order to clarify some fundamental concepts for understanding the problem of discrimination in data analysis, and thus allowing them to get later into the experiments and the study of the adopted tools with a greater awareness.
\item Chapter 3: \textbf{Technical Preliminaries}. The aim of this chapter is to provide the reader with preliminary notions about the technical knowledge necessary to understand the functioning of the adopted tools, which are described later in Chapter~\ref{chapter:tools_for_assessing_fairness}.
\item Chapter 4: \textbf{Sociological Perspective}. The aim of this chapter is to provide the reader with a sociological background by reporting information about gender gap in the society.
\item Chapter 5: \textbf{Techniques}. The aim of this chapter is to provide the reader with an overview on the tools adopted in our experiments.
\item Chapter 6: \textbf{Experiments}. The aim of this chapter is to describe the experiments conducted using the tools introduced in Chapter~\ref{chapter:tools_for_assessing_fairness}, in order to verify the presence (and the nature) of bias in our datasets and discuss their fairness, according to the concepts introduced in Chapter~\ref{chapter:socio-ethical_preliminaries} and the sociological background explored in Chapter~\ref{chapter:sociological_perspective}.
\item Chapter 7: \textbf{Results}. The aim of this chapter is to discuss the results obtained from our experiments described in Chapter~\ref{chapter:experiments}, mixing them with the sociological background depicted in Chapter~\ref{chapter:sociological_perspective}, and recalling some preliminaries exposed in Chapter~\ref{chapter:socio-ethical_preliminaries} and Chapter~\ref{chapter:technical_preliminaries} when needed.
\item Chapter 8: \textbf{Conclusions \& Future Work}. The aim of this chapter is to briefly draw the conclusions of our research, summarizing what has been done during the thesis work and providing some ideas for further research in this area.
\end{itemize}
